# Wiigwaas
#### Experimental Linguistics & Fieldwork (ELF) Lab @ UBC
Director: Dr. Chris Hammerly  
Author: Anna Stacey  

## Introduction
Wiigwaas contains the necessary pieces for visual world eye-tracking experiments.  The experimental code is written in Python and run in [PsychoPy](https://www.psychopy.org).  We use a Tobii eyetracker (Tobii Pro Fusion), recording the gaze data in [Tobii Pro Lab](https://www.tobii.com/products/software/behavior-research-software/tobii-pro-lab) via the [Titta](https://github.com/marcus-nystrom/Titta/tree/master) package.

The experiment that this code was created for involves a number of pictures being displayed on the screen, audio being played, and the participant selecting one of the images.  This process repeats for the desired number of trials.  However, this code can be used as a starting point and can be modified to work with different experimental flows.

Currently, the code is designed to work with input from either a mouse or touch screen (tested using X monitor).  The code could be modified to work with other input sources, and indeed we are planning on adding Cedrus support in future.

## `wiigwaas.py`
This is the code that manages the overall experimental flow.  It is specific to our example experiment, but provides an example of how to make use of the various functionality provided by the other modules.

It's the control centre from which each trial is structured, but it doesn't contain any reuseable functions.

### Running the Experiment

Open up `wiigwaas.py` in the Coder window of PsychoPy.  Hit the play button to run it.  If you're including the eye-tracking component (you can turn this off [here](#configpy)), you'll need to use Tobii Pro Lab for recording.

#### Setting Up Tobii Pro Lab
As noted in the Titta docs, if you want to record in Tobii Pro Lab, there are a few steps involved to get the software ready.
1. Open Tobii Pro Lab.
2. Create New Project > External Presenter Project
    - You can name the project whatever; we just stick with the default numbered names.
3. Switch to the Record tab.  
4. Now you can run the experiment from PsychoPy.
5. When the experiment is done, swtich to the Analyze tab to view the recording.

Note that if you are not changing the participant number each time you run the experiment, you will need to always be creating a new project or Tobii Pro Lab will complain that that participant already exists in the current project.

## `display_resources.py`
This module contains functions related to what shows up on the screen.

#### Input-Related Functions
|Function|Inputs|Outputs|
|---|---|---|
|`check_for_input_on_images`: a general function (works for any input type) to check if the user has selected one of the images.|<ul> <li>`mouse`: the mouse object in the project</li><li>`images`: a list of ImageStim objects, containing the set of images which you want to check for input on</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`selected_image`: an ImageStim object which is the image from the given set that was selected. May be `None`.</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`_check_for_click_on_images`: a private function (to be called by the more general `check_for_input_on_images`) to check if the user has selected one of the images *via mouse input*.|<ul> <li>`mouse`: the mouse object in the project</li><li>`images`: a list of ImageStim objects, containing the set of images which you want to check for input on</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`clicked_image`: the image from the given set that was selected. May be `None`.</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`_check_for_tap_on_images`: a private function (to be called by the more general `check_for_input_on_images`) to check if the user has selected one of the images *via touch input*.|<ul> <li>`mouse`: the mouse object in the project</li><li>`images`: a list of ImageStim objects, containing the set of images which you want to check for input on</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`tapped_image`: the image from the given set that was selected. May be `None`.</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`check_for_input_anywhere`: a general function (works for any input type) to check if the user has selected one of the images.|<ul> <li>`mouse`: the mouse object in the project</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`input_received`: a boolean indicating whether or not any input occurred</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`_check_for_click_anywhere`: a private function (to be called by the more general `check_for_input_on_images`) to check if the user has selected one of the images *via mouse input*.|<ul> <li>`mouse`: the mouse object in the project</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`clicked`: a boolean indicating whether or not a click was received</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`_check_for_tap_anywhere`: a private function (to be called by the more general `check_for_input_on_images`) to check if the user has selected one of the images *via touch input*.|<ul> <li>`mouse`: the mouse object in the project</li><li>`prev_mouse_location`: the variable that tracks where the mouse was last. This is needed in case you're using touch input, because it's processed as a hovering mouse.</li></ul>|<ul><li>`tapped`: a boolean indicating whether or not a tap was received</li><li>`prev_mouse_location`: the updated mouse location to track</li></ul>|
|`clear_clicks_and_events`: a function that resets PsychoPy's tracking of clicks and other events.  Psychopy [recommends](https://psychopy.org/api/event.html#psychopy.event.Mouse.getPressed) this at stimulus onset.|<ul> <li>`mouse`: the mouse object in the project</li></ul>||
|`handle_input_on_stimulus`: a function that deals with the user selection of a stimulus.  Given the predetermined selected ImageStim, it draws the selection box around that image and adds the checkmark button to that image.  This function has a lot of input parameters because it involves re-drawing the whole stimuli screen.|<ul><li>`selected_image`: an ImageStim object indicating which image from the given set was selected. This function is only called when an image has been selected, so this should not be None.</li><li>`images`: a list of ImageStim objects, containing the set of stimuli which may have been selected.</li><li> `checkmarks`: a list of ImageStim objects, containing a checkmark associated with each image</li><li>`selection_box`: a PsychoPy Rect object for the box around the selected image. This function will assign it the correct position based on the selected image.</li><li>`repeat_icon`: an ImageStim for the repeat button</li><li>`trial_clock`: a Psychopy core.Clock object associated with the present trial</li><li>`clicks`: a list recording each click. Each item in the list is itself a list containing a) a string identifying what was clicked, and b) a float indicating the time of the click in seconds (based on the `trial_clock`).</li><li>`recorder`: the recorder object that is tracking the gaze.  As usual, this is just being passed on for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed (again, passed on to inform the recorder).  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li></ul>|<ul><li>`checkmark`: the ImageStim object for the checkmark which is now displayed on-screen, associated with the selected image|

#### Display-Related Functions
|Function|Inputs|Outputs|
|---|---|---|
|`display_blank_screen`: a function that changes the display to just a blank screen.|<ul> <li>`recorder`: the recorder object that is tracking the gaze.  This is just being passed on, not used in this function per se, for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed.  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li></ul>||
|`display_buffer_screen`: a function that changes the display to just a buffer screen, where the experiment will remain until user input is received.  The buffer screen is used as a kind of pause in between trials.|<ul> <li>`recorder`: the recorder object that is tracking the gaze.  This is just being passed on, not used in this function per se, for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed.  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li><li>`mouse`: the mouse object in the project</li><li>`buffer_text`: a string containing the text to be displayed for the duration of the buffer screen</li><li>`quit_function`: the function that will be run if the user quits while the buffer screen is being displayed</li></ul>||
|`display_fixation_cross_screen`: a function that changes the display to a fixation cross.  Note that any drift check is not controlled from here - this function simply changes the display.|<ul> <li>`recorder`: the recorder object that is tracking the gaze.  This is just being passed on, not used in this function per se, for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed.  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li></ul>||
|`display_stimuli_screen`: a function that changes the display to show the stimuli objects|<ul> <li>`recorder`: the recorder object that is tracking the gaze.  This is just being passed on, not used in this function per se, for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed.  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li><li>`stimuli_list`: the list of stimuli as ImageStim objects (which should already have their position etc. assigned), each of which will be drawn on screen.</li></ul>||
|`display_text_screen`: a function that changes the display to show some text|<ul> <li>`recorder`: the recorder object that is tracking the gaze.  This is just being passed on, not used in this function per se, for the sake of updating the recorder on what's on screen.</li><li>`media_info`: the list of media being used in the project, from which we can find the screen that's to be displayed.  The format is a dictionary with image name keys and response values.</li><li>`main_window`: the window object for the project, whose display will be changed.</li><li>`text_to_display`: a string containing the text to be displayed for the duration of the screen</li><li>`display_name`: a string indicating an identifiable name for this screen, to be provided to the recorder</li></ul>||

#### Miscellaneous Functions
|Function|Inputs|Outputs|
|---|---|---|
|`display_subj_ID_dialog`: a function that brings up a GUI input box to enter the participant's ID number in to||<ul><li>`subj_ID`: the provided ID number of the current participant</li></ul>|
|`get_images`: a function that, given the image file names, retrives them and gets them into a PsychoPy-appropriate format|<ul><li>`image_file_names`: a list of strings indicating the name of each stimuli within the `visualStims` folder</li><li>`image_size`: an integer indicating the desired size of each stimulus image in pixels (with the presumption of squares, so that this is both the width and height)</li><li>`checkmark_size`: an integer indicating the desired size of the checkmark image in pixels (again, both the width and height)</li><li>`repeat_icon_size`: an integer indicating the desired size of the repeat button image in pixels (again, both the width and height)</li><li>`main_window`: the window object for the project, where each of these images will eventually be displayed</li></ul>|<ul><li>`images`: a list of the stimuli as ImageStim objects</li>`checkmarks`: a list of the checkmarks as ImageStim objects.  Though they appear identical, we need one checkmark ImageStim for each stimulus image.</li><li>`repeat_icon`: an ImageStim for the repeat button</li><li>`selection_box`: a PsychoPy Rect object for the box around the selected image</li></ul>|
|`get_random_image_order`: a function that gives a random ordering for the given number of stimuli images.  For example, if there are three stimuli to be displayed, it will randomly return one of [0,1,2], [0,2,1], [1,0,2], [1,2,0], [2,0,1] or [2,1,0].|<ul><li>`number_of_images`: an integer indicating how many images we are ordering</li></ul>|<ul><li>`randomly_ordered_list`: a list of length `number_of_images`, containing the integers from 0 to 1 - `number_of_images`, randomly ordered</li></ul>|

## `eye_tracking_resources.py`
This module contains functions related to the eye-tracking process, including communication with Titta (and by extension, Tobii Pro Lab).

## `config.py`
This is where project-level constants are defined.  Various modules are expecting the following values to be defined:

|Constant name|Value|
|---|---|
|`EYE_TRACKING_ON`|a boolean indicating whether you want the experiment to run with or without including the eye-tracking steps.  This essentially allows a test mode where you can check how other parts of the experiment are working without worrying about the eye-tracking side.|
|`WINDOW_WIDTH`|an integer indicating the width of the screen the display will be shown on, in pixels.|
|`WINDOW_HEIGHT`|an integer indicating the height of the screen the display will be shown on, in pixels.|
|`USER_INPUT_DEVICE`|a string indicating the source of user input.  Currently supported values are 'mouse' or 'touch'.|
